{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "import markitdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"AKIASBGQLOUGU4LBU7PD\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "S3_BUCKET_NAME = \"scrapedimages\"\n",
    "S3_REGION = \"us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate a hash for image validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to upload files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_path, bucket_name, s3_key, aws_access_key, aws_secret_key, region):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, s3_key)\n",
    "        s3_url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}\"\n",
    "        print(f\"Uploaded {file_path} to {s3_url}\")\n",
    "        return s3_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(url, save_directory=\"images\"):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    image_urls = []\n",
    "    hashes = set()\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(save_directory, img_name)\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "\n",
    "                img_hash = generate_hash(img_path)\n",
    "                if img_hash in hashes:\n",
    "                    print(f\"Duplicate image skipped: {img_url}\")\n",
    "                    continue\n",
    "\n",
    "                hashes.add(img_hash)\n",
    "                s3_key = f\"scraped_images/{img_name}\"\n",
    "                s3_url = upload_to_s3(img_path, S3_BUCKET_NAME, s3_key, AWS_ACCESS_KEY, AWS_SECRET_KEY, S3_REGION)\n",
    "                if s3_url:\n",
    "                    image_urls.append(s3_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tables(url, output_directory=\"tables\"):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_files = []\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            file_path = os.path.join(output_directory, f\"table_{i + 1}.csv\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            table_files.append(file_path)\n",
    "            print(f\"Saved table {i + 1} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse table {i + 1}: {e}\")\n",
    "    return table_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_metadata(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    metadata = {\n",
    "        \"title\": soup.title.string if soup.title else \"No Title\",\n",
    "        \"meta_tags\": [\n",
    "            {\"name\": tag.get(\"name\"), \"content\": tag.get(\"content\")}\n",
    "            for tag in soup.find_all(\"meta\")\n",
    "        ]\n",
    "    }\n",
    "    metadata_file = \"metadata.json\"\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "    return metadata_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save markdown file using markitdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown(metadata_file, table_files, image_urls, output_file=\"output.md\"):\n",
    "    doc = md.Document()\n",
    "\n",
    "    # Add Metadata\n",
    "    doc.add_heading(\"Metadata\", level=1)\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "        doc.add_codeblock(json.dumps(metadata, indent=4), language=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Images, Tables and saving Markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = md.Document()\n",
    "\n",
    "doc.add_heading(\"Images\", level=1)\n",
    "\n",
    "for img_url in image_urls:\n",
    "        doc.add_paragraph(f\"![Image]({img_url})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
